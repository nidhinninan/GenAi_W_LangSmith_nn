{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48dcfa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m load_dotenv()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m## Langsmith Tracking\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLANGCHAIN_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_TRACING_V2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/CSCE5720/lib/python3.11/os.py:684\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[1;32m    683\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)\n\u001b[0;32m--> 684\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     putenv(key, value)\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/anaconda3/envs/CSCE5720/lib/python3.11/os.py:758\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(value):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mencode(encoding, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51abec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully authenticated with AWS.\n",
      "---------------------------------------\n",
      "Account ID: 322691663228\n",
      "User ID:    AIDAUWIPMAV6OANS3KFHN\n",
      "ARN:        arn:aws:iam::322691663228:user/langsmith-bedrock-user\n",
      "Using AWS Profile: langsmith-bedrock-user\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an STS client\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "try:\n",
    "    # Call the get_caller_identity() method\n",
    "    identity = sts_client.get_caller_identity()\n",
    "    \n",
    "    print(\"✅ Successfully authenticated with AWS.\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Account ID: {identity['Account']}\")\n",
    "    print(f\"User ID:    {identity['UserId']}\")\n",
    "    print(f\"ARN:        {identity['Arn']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error authenticating with AWS: {e}\")\n",
    "    print(\"Please check your credentials (e.g., run 'aws configure' in your terminal).\")\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "# Check the profile name boto3 is configured to use\n",
    "print(f\"Using AWS Profile: {session.profile_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb6f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<botocore.client.BedrockRuntime object at 0x71e1dcb41590> bedrock_client=<botocore.client.Bedrock object at 0x71e1dc857790> region_name='us-east-1' model_id='meta.llama3-8b-instruct-v1:0' model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"meta.llama3-8b-instruct-v1:0\", # Or any other model you have access to\n",
    "    region_name=\"us-east-1\"      # Or your preferred region\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc21263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685235cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a type of artificial intelligence (AI) that can create new, original content, such as text, images, music, or videos, rather than simply processing or analyzing existing data. Generative AI models use algorithms and machine learning techniques to generate new data that is similar in style, tone, and quality to existing data, but is not necessarily identical to it.\\n\\nGenerative AI has many applications, including:\\n\\n1. **Art and design**: Generative AI can create new art, music, and designs, such as generating new images, music tracks, or even entire albums.\\n2. **Content creation**: Generative AI can help create new content, such as blog posts, articles, or social media posts, by generating text based on a given topic or style.\\n3. **Data augmentation**: Generative AI can be used to augment existing datasets by generating new, synthetic data that is similar to the original data.\\n4. **Chatbots and conversational AI**: Generative AI can be used to create more natural and human-like conversations by generating responses to user input.\\n5. **Music and audio**: Generative AI can be used to create new music, sound effects, or even entire audio tracks.\\n\\nSome examples of generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs)**: GANs are a type of deep learning algorithm that consists of two neural networks that work together to generate new data.\\n2. **Transformers**: Transformers are a type of neural network that are particularly well-suited for natural language processing and can be used to generate text, such as chatbot responses or entire articles.\\n3. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that can be used to generate new data by learning the underlying distribution of the data.\\n\\nThe benefits of generative AI include:\\n\\n1. **Increased efficiency**: Generative AI can automate the process of creating new content, reducing the need for human labor and increasing efficiency.\\n2. **Improved accuracy**: Generative AI can generate new data that is similar in style and quality to existing data, reducing the need for human editing and improving accuracy.\\n3. **New creative possibilities**: Generative AI can enable new forms of creative expression and collaboration between humans and machines.\\n\\nHowever, generative AI also raises important ethical and legal questions, such as:\\n\\n1. **Authorship and ownership**: Who owns the generated content, and who is responsible for its creation and distribution?\\n2. **Bias and fairness**:' additional_kwargs={'usage': {'prompt_tokens': 17, 'completion_tokens': 512, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 529}, 'stop_reason': 'length', 'thinking': {}, 'model_id': 'meta.llama3-8b-instruct-v1:0'} response_metadata={'usage': {'prompt_tokens': 17, 'completion_tokens': 512, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 529}, 'stop_reason': 'length', 'thinking': {}, 'model_id': 'meta.llama3-8b-instruct-v1:0', 'model_provider': 'bedrock', 'model_name': 'meta.llama3-8b-instruct-v1:0'} id='lc_run--a584b15e-9a5c-4511-bd06-f77d8e3e5e66-0' usage_metadata={'input_tokens': 17, 'output_tokens': 512, 'total_tokens': 529, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395bf348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5bc3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Langsmith is a fascinating topic!\\n\\nLangsmith is an AI-powered language engineering platform that enables developers to build, train, and deploy custom language models for various applications. The platform provides a range of tools and services to help developers create high-quality language models that can be used for natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and more.\\n\\nSome of the key features of Langsmith include:\\n\\n1. **Model training**: Langsmith allows developers to train custom language models using their own datasets, which can be used for specific applications.\\n2. **Model deployment**: Once trained, Langsmith enables developers to deploy their models in various environments, including cloud, on-premises, and mobile devices.\\n3. **Model management**: Langsmith provides a centralized platform for managing multiple language models, making it easy to track performance, update models, and scale deployments.\\n4. **Integration with popular frameworks**: Langsmith supports integration with popular frameworks such as TensorFlow, PyTorch, and scikit-learn, making it easy to incorporate language models into existing workflows.\\n5. **Pre-built models**: Langsmith offers a range of pre-built language models that can be used out-of-the-box for common NLP tasks, such as text classification, sentiment analysis, and language translation.\\n\\nLangsmith is particularly useful for developers working on NLP projects, as it simplifies the process of building and deploying custom language models. By leveraging Langsmith, developers can focus on developing their applications rather than spending time on model training and deployment.\\n\\nWould you like to know more about Langsmith or is there a specific aspect you'd like me to elaborate on?\" additional_kwargs={'usage': {'prompt_tokens': 38, 'completion_tokens': 336, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 374}, 'stop_reason': 'stop', 'thinking': {}, 'model_id': 'meta.llama3-8b-instruct-v1:0'} response_metadata={'usage': {'prompt_tokens': 38, 'completion_tokens': 336, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 374}, 'stop_reason': 'stop', 'thinking': {}, 'model_id': 'meta.llama3-8b-instruct-v1:0', 'model_provider': 'bedrock', 'model_name': 'meta.llama3-8b-instruct-v1:0'} id='lc_run--bb150b5b-19ff-4d76-996e-7299813783c9-0' usage_metadata={'input_tokens': 38, 'output_tokens': 336, 'total_tokens': 374, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98943cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a437deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is an open-source, cloud-based platform that enables data scientists and engineers to build, deploy, and manage machine learning models at scale. It provides a range of tools and services to streamline the machine learning workflow, from data preparation to model deployment.\n",
      "\n",
      "Some of the key features of Langsmith include:\n",
      "\n",
      "1. **Model Management**: Langsmith allows users to manage their machine learning models in a centralized repository, making it easy to track versions, collaborate with team members, and deploy models to production.\n",
      "2. **Automated Model Training**: Langsmith provides automated model training capabilities, allowing users to train models on large datasets without manual intervention.\n",
      "3. **Model Serving**: Langsmith provides a scalable model serving platform that can handle high-traffic and high-volume requests, making it suitable for production environments.\n",
      "4. **Data Preparation**: Langsmith includes data preparation tools that enable users to preprocess and transform data for machine learning model training.\n",
      "5. **Collaboration**: Langsmith provides features for collaboration, such as version control, commenting, and @mentioning, making it easy for teams to work together on machine learning projects.\n",
      "6. **Integration**: Langsmith integrates with popular machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn, as well as cloud platforms like AWS, GCP, and Azure.\n",
      "\n",
      "Langsmith is particularly useful for organizations that need to manage multiple machine learning models, collaborate with teams, and deploy models to production environments. It is also suitable for data scientists and engineers who want to streamline their machine learning workflow and focus on building and deploying models rather than managing infrastructure.\n",
      "\n",
      "Overall, Langsmith is a powerful platform that can help organizations accelerate their machine learning journey and achieve greater efficiency and scalability.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE5720",
   "language": "python",
   "name": "csce5720"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
